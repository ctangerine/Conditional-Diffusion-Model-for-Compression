{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22278f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "# sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), './')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9a101c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "COCO_ROOT_DIR = r\"D:\\temp_dataset\\coco\\images\\train2017.1\\train2017\"  # Thư mục gốc chứa COCO\n",
    "TRAIN_IMAGES_SUBDIR = \"\"     # Thư mục con chứa ảnh train2017\n",
    "TRAIN_ANNOTATIONS_FILENAME = \"captions_train2017.json\" # File chú thích cho train2017\n",
    "\n",
    "PATCH_SIZE = (256, 256)  # Kích thước patch mong muốn (height, width)\n",
    "BATCH_SIZE = 64          # Kích thước batch cho DataLoader\n",
    "NUM_WORKERS = 0         # Số luồng để tải dữ liệu\n",
    "\n",
    "# --- Xây dựng đường dẫn đầy đủ ---\n",
    "train_images_path = os.path.join(COCO_ROOT_DIR, TRAIN_IMAGES_SUBDIR)\n",
    "train_annotations_path = os.path.join(r\"D:\\temp_dataset\\coco\\images\\train2017.1\\annotations_trainval2017\\annotations\", TRAIN_ANNOTATIONS_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a3cc2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 images in D:\\ds_coco_patches\n",
      "Batch size: 16\n",
      "Batch shape: torch.Size([16, 3, 256, 256])\n",
      "Estimated memory usage of the batch on CUDA: 12.00 MB\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "class ImageOnlyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A dataset that loads only images from a directory (no labels).\n",
    "    All images are assumed to be the same size.\n",
    "    \"\"\"\n",
    "    def __init__(self, image_dir, transform=None, cache_size=0, max_images=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_dir (str): Path to directory containing images\n",
    "            transform (callable, optional): Transform to apply to images\n",
    "            cache_size (int): Number of images to cache in memory (0 for no caching)\n",
    "            max_images (int): Limit number of images loaded from the folder\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.ToTensor(),  # Converts images to tensors [0-1]\n",
    "        ])\n",
    "        self.cache_size = cache_size\n",
    "        self.cache = {}\n",
    "        \n",
    "        # Get all image paths but don't load them yet\n",
    "        self.image_paths = []\n",
    "        for ext in ['*.jpg', '*.jpeg', '*.png', '*.bmp']:\n",
    "            self.image_paths.extend(glob.glob(os.path.join(image_dir, ext)))\n",
    "            \n",
    "        # Sort for reproducibility\n",
    "        self.image_paths.sort()\n",
    "        \n",
    "        # Only keep a limited number of images\n",
    "        if max_images is not None:\n",
    "            self.image_paths = self.image_paths[:max_images]\n",
    "        \n",
    "        print(f\"Found {len(self.image_paths)} images in {image_dir}\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Check if image is in cache\n",
    "        if idx in self.cache:\n",
    "            return self.cache[idx]\n",
    "        \n",
    "        # Load image only when needed\n",
    "        image_path = self.image_paths[idx]\n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "                image = img.convert('RGB')\n",
    "                if self.transform:\n",
    "                    image = self.transform(image)\n",
    "                if self.cache_size > 0 and len(self.cache) < self.cache_size:\n",
    "                    self.cache[idx] = image\n",
    "                return image\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {image_path}: {e}\")\n",
    "            return torch.zeros(3, 256, 256)  # Placeholder on failure\n",
    "\n",
    "    def clear_cache(self):\n",
    "        self.cache.clear()\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "# Setup the dataset and dataloader\n",
    "def get_coco_patches_loader(\n",
    "    data_dir=\"D:\\\\ds_coco_patches\", \n",
    "    batch_size=64,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    shuffle=True,\n",
    "    cache_size=0,\n",
    "    max_images=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a DataLoader for loading images from the specified directory.\n",
    "    \"\"\"\n",
    "    dataset = ImageOnlyDataset(data_dir, cache_size=cache_size, max_images=max_images)\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory\n",
    "    )\n",
    "    \n",
    "    return dataloader, dataset\n",
    "\n",
    "\n",
    "# ------------------------ Example usage ------------------------\n",
    "\n",
    "BATCH_SIZE = 16  # Adjust based on your GPU memory\n",
    "NUM_WORKERS = 0  # 0 = no multiprocessing\n",
    "MAX_IMAGES = 100  # Limit to first 3000 images\n",
    "\n",
    "train_loader, train_dataset = get_coco_patches_loader(\n",
    "    data_dir=\"D:\\\\ds_coco_patches\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    cache_size=0,\n",
    "    max_images=MAX_IMAGES\n",
    ")\n",
    "\n",
    "# Move a batch to CUDA and measure its size\n",
    "for images in train_loader:\n",
    "    images = images.cuda()  # Move to CUDA\n",
    "    print(f\"Batch size: {images.size(0)}\")\n",
    "    print(f\"Batch shape: {images.shape}\")  # Should be [B, 3, H, W]\n",
    "    \n",
    "    # Calculate memory usage\n",
    "    batch_memory = images.element_size() * images.nelement() / (1024 ** 2)\n",
    "    print(f\"Estimated memory usage of the batch on CUDA: {batch_memory:.2f} MB\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0699def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([16, 3, 256, 256])\n",
      "Estimated batch size in MB: 12.00\n"
     ]
    }
   ],
   "source": [
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"Batch shape: {sample_batch.shape}\")\n",
    "print(f\"Estimated batch size in MB: {sample_batch.element_size() * sample_batch.nelement() / 1024**2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b1e0375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing HyperPrior on device: cuda\n",
      "DiffusionManager initialized on device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python installation\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\python installation\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from compressor.compressor import Compressor\n",
    "from decompressor.diffusion_manager import DiffusionManager\n",
    "from decompressor.unet_module import UnetModule\n",
    "\n",
    "\n",
    "compressor = Compressor(\n",
    "    in_channel=3,\n",
    "    out_channel=3,\n",
    "    base_channel=64,\n",
    "    bitrate_conditional=True,\n",
    ")\n",
    "\n",
    "unet_module = UnetModule()\n",
    "\n",
    "model = DiffusionManager(\n",
    "    encoder=compressor,\n",
    "    u_net=unet_module,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf296129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 80.36 MB\n"
     ]
    }
   ],
   "source": [
    "def get_model_size_in_mb(model):\n",
    "    total_size = sum(param.element_size() * param.nelement() for param in model.parameters())\n",
    "    total_size += sum(buffer.element_size() * buffer.nelement() for buffer in model.buffers())\n",
    "    return total_size / (1024 ** 2)  # Convert bytes to MB\n",
    "\n",
    "model_size_mb = get_model_size_in_mb(model)\n",
    "print(f\"Model size: {model_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40dc12ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3ff6f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on cuda\n",
      "Training for 5 epochs with batch size 16\n",
      "\n",
      "Epoch 1/5\n",
      "len train_loader: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor size: 12.00 MB\n",
      "\u001b[1;35m2025-05-12 12:32:33 - CompressorLogger - CRITICAL - input image shape: torch.Size([16, 3, 256, 256])\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  14%|█▍        | 1/7 [00:18<01:50, 18.48s/it, loss=574.2792, prior_loss=72.4623]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor size: 12.00 MB\n",
      "\u001b[1;35m2025-05-12 12:32:52 - CompressorLogger - CRITICAL - input image shape: torch.Size([16, 3, 256, 256])\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  14%|█▍        | 1/7 [00:20<02:03, 20.66s/it, loss=574.2792, prior_loss=72.4623]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 9.50 GiB is allocated by PyTorch, and 91.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 56\u001b[0m\n\u001b[0;32m     53\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Forward pass through diffusion model\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m total_loss, prior_loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m images\n\u001b[0;32m     60\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "File \u001b[1;32md:\\python installation\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\python installation\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32me:\\DUT Courses\\Academic year 4\\semester 2\\PBL\\PBL7 CDC Compression\\decompressor\\diffusion_manager.py:161\u001b[0m, in \u001b[0;36mDiffusionManager.forward\u001b[1;34m(self, input_tensor)\u001b[0m\n\u001b[0;32m    158\u001b[0m bitrate_scalar_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((batch_size,), torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem(), device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# Run encoder\u001b[39;00m\n\u001b[1;32m--> 161\u001b[0m encoder_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbitrate_condition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbitrate_scalar_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m context, bpp, _, _ \u001b[38;5;241m=\u001b[39m encoder_output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m], encoder_output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbpp\u001b[39m\u001b[38;5;124m\"\u001b[39m], encoder_output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantize_latent\u001b[39m\u001b[38;5;124m\"\u001b[39m], encoder_output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantize_hyper_latent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    165\u001b[0m \u001b[38;5;66;03m# Generate random timesteps\u001b[39;00m\n",
      "File \u001b[1;32md:\\python installation\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\python installation\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32me:\\DUT Courses\\Academic year 4\\semester 2\\PBL\\PBL7 CDC Compression\\compressor\\compressor.py:514\u001b[0m, in \u001b[0;36mCompressor.forward\u001b[1;34m(self, input_image, bitrate_condition)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;66;03m# 1. Mã hóa\u001b[39;00m\n\u001b[0;32m    513\u001b[0m logger\u001b[38;5;241m.\u001b[39mcritical(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput image shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_image\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 514\u001b[0m quantize_latent, quantize_hyper_latent, state4bpp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbitrate_condition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[38;5;66;03m# 2. Tính BPP\u001b[39;00m\n\u001b[0;32m    517\u001b[0m bpp_estimate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbpp(input_image\u001b[38;5;241m.\u001b[39mshape, state4bpp)\n",
      "File \u001b[1;32me:\\DUT Courses\\Academic year 4\\semester 2\\PBL\\PBL7 CDC Compression\\compressor\\compressor.py:273\u001b[0m, in \u001b[0;36mCompressor.encode\u001b[1;34m(self, input_image, bitrate_condition)\u001b[0m\n\u001b[0;32m    269\u001b[0m downsample_layer \u001b[38;5;241m=\u001b[39m encoder_block[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_shape\u001b[38;5;241m.\u001b[39mappend(current_features\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m--> 273\u001b[0m current_features \u001b[38;5;241m=\u001b[39m \u001b[43mresnet_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;66;03m# Áp dụng điều kiện bitrate nếu được kích hoạt\u001b[39;00m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbitrate_conditional:\n",
      "File \u001b[1;32md:\\python installation\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\python installation\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32me:\\DUT Courses\\Academic year 4\\semester 2\\PBL\\PBL7 CDC Compression\\network_components\\resnet_block.py:62\u001b[0m, in \u001b[0;36mResnetBlock.forward\u001b[1;34m(self, input, time_tensor)\u001b[0m\n\u001b[0;32m     60\u001b[0m conv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresnes_block:\n\u001b[1;32m---> 62\u001b[0m     conv \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m conv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     time_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_tensor)\n",
      "File \u001b[1;32md:\\python installation\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\python installation\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32me:\\DUT Courses\\Academic year 4\\semester 2\\PBL\\PBL7 CDC Compression\\network_components\\layer_normalization.py:36\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     34\u001b[0m var \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mvar(\u001b[38;5;28minput\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, unbiased\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     35\u001b[0m mean \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;28minput\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(var \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 9.50 GiB is allocated by PyTorch, and 91.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import tqdm\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "NUM_WORKERS = 0 if torch.cuda.is_available() else 0\n",
    "\n",
    "# Create optimizer for both compressor and UNet\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.encoder.parameters()},\n",
    "    {'params': model.u_net.parameters()}\n",
    "], lr=1e-4)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 5\n",
    "log_interval = 50  # Log every 50 batches\n",
    "save_interval = 1   # Save checkpoint every epoch\n",
    "\n",
    "# Create directory for saving checkpoints\n",
    "save_dir = os.path.join(\"checkpoints\", \n",
    "                        f\"cdc_training_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Training statistics\n",
    "train_losses = []\n",
    "prior_losses = []\n",
    "\n",
    "# Move models to CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Starting training on {device}\")\n",
    "print(f\"Training for {num_epochs} epochs with batch size {BATCH_SIZE}\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    train_dataset.clear_cache()  # Clear cache at the start of each epoch\n",
    "    epoch_losses = []\n",
    "    epoch_prior_losses = []\n",
    "\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"len train_loader: {len(train_loader)}\")\n",
    "    \n",
    "    # Create tqdm progress bar\n",
    "    progress_bar = tqdm.tqdm(enumerate(train_loader), total=len(train_loader),\n",
    "                             desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    model.train()  # Set model to training mode\n",
    "    \n",
    "    for batch_idx, images in progress_bar:\n",
    "        images = images.to(device)  # Move images to GPU\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass through diffusion model\n",
    "        total_loss, prior_loss = model(images)\n",
    "        \n",
    "        \n",
    "        del images\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        # Optional: gradient clipping for stability\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Record losses\n",
    "        epoch_losses.append(total_loss.mean().item())\n",
    "        epoch_prior_losses.append(prior_loss.item())\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f\"{epoch_losses[-1]:.4f}\",\n",
    "            'prior_loss': f\"{epoch_prior_losses[-1]:.4f}\"\n",
    "        })\n",
    "        \n",
    "        # Log periodically\n",
    "        if batch_idx % log_interval == 0 and batch_idx > 0:\n",
    "            print(f\"\\nBatch {batch_idx}/{len(train_loader)}, \"\n",
    "                  f\"Loss: {sum(epoch_losses[-log_interval:]) / log_interval:.4f}, \"\n",
    "                  f\"Prior Loss: {sum(epoch_prior_losses[-log_interval:]) / log_interval:.4f}\")\n",
    "            \n",
    "        time.sleep(5)\n",
    "    \n",
    "    # Calculate average epoch loss\n",
    "    avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "    avg_prior_loss = sum(epoch_prior_losses) / len(epoch_prior_losses)\n",
    "    train_losses.append(avg_loss)\n",
    "    prior_losses.append(avg_prior_loss)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs} completed, \"\n",
    "          f\"Avg Loss: {avg_loss:.4f}, \"\n",
    "          f\"Avg Prior Loss: {avg_prior_loss:.4f}\")\n",
    "    \n",
    "    # Save checkpoints\n",
    "    if (epoch + 1) % save_interval == 0:\n",
    "        checkpoint_path = os.path.join(save_dir, f\"checkpoint_epoch_{epoch+1}.pt\")\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'encoder_state_dict': model.encoder.state_dict(),\n",
    "            'unet_state_dict': model.u_net.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Saved checkpoint to {checkpoint_path}\")\n",
    "\n",
    "# Plot training curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Total Loss')\n",
    "plt.plot(range(1, num_epochs + 1), prior_losses, label='Prior Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
