{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22278f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "# sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), './')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9a101c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "COCO_ROOT_DIR = r\"D:\\temp_dataset\\coco\\images\\train2017.1\\train2017\"  # Thư mục gốc chứa COCO\n",
    "TRAIN_IMAGES_SUBDIR = \"\"     # Thư mục con chứa ảnh train2017\n",
    "TRAIN_ANNOTATIONS_FILENAME = \"captions_train2017.json\" # File chú thích cho train2017\n",
    "\n",
    "PATCH_SIZE = (256, 256)  # Kích thước patch mong muốn (height, width)\n",
    "BATCH_SIZE = 64          # Kích thước batch cho DataLoader\n",
    "NUM_WORKERS = 0         # Số luồng để tải dữ liệu\n",
    "\n",
    "# --- Xây dựng đường dẫn đầy đủ ---\n",
    "train_images_path = os.path.join(COCO_ROOT_DIR, TRAIN_IMAGES_SUBDIR)\n",
    "train_annotations_path = os.path.join(r\"D:\\temp_dataset\\coco\\images\\train2017.1\\annotations_trainval2017\\annotations\", TRAIN_ANNOTATIONS_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a3cc2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3000 images in D:\\ds_coco_patches\n",
      "Batch size: 32\n",
      "Batch shape: torch.Size([32, 3, 256, 256])\n",
      "Estimated memory usage of the batch on CUDA: 24.00 MB\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "class ImageOnlyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A dataset that loads only images from a directory (no labels).\n",
    "    All images are assumed to be the same size.\n",
    "    \"\"\"\n",
    "    def __init__(self, image_dir, transform=None, cache_size=0, max_images=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_dir (str): Path to directory containing images\n",
    "            transform (callable, optional): Transform to apply to images\n",
    "            cache_size (int): Number of images to cache in memory (0 for no caching)\n",
    "            max_images (int): Limit number of images loaded from the folder\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.ToTensor(),  # Converts images to tensors [0-1]\n",
    "        ])\n",
    "        self.cache_size = cache_size\n",
    "        self.cache = {}\n",
    "        \n",
    "        # Get all image paths but don't load them yet\n",
    "        self.image_paths = []\n",
    "        for ext in ['*.jpg', '*.jpeg', '*.png', '*.bmp']:\n",
    "            self.image_paths.extend(glob.glob(os.path.join(image_dir, ext)))\n",
    "            \n",
    "        # Sort for reproducibility\n",
    "        self.image_paths.sort()\n",
    "        \n",
    "        # Only keep a limited number of images\n",
    "        if max_images is not None:\n",
    "            self.image_paths = self.image_paths[:max_images]\n",
    "        \n",
    "        print(f\"Found {len(self.image_paths)} images in {image_dir}\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Check if image is in cache\n",
    "        if idx in self.cache:\n",
    "            return self.cache[idx]\n",
    "        \n",
    "        # Load image only when needed\n",
    "        image_path = self.image_paths[idx]\n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "                image = img.convert('RGB')\n",
    "                if self.transform:\n",
    "                    image = self.transform(image)\n",
    "                if self.cache_size > 0 and len(self.cache) < self.cache_size:\n",
    "                    self.cache[idx] = image\n",
    "                return image\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {image_path}: {e}\")\n",
    "            return torch.zeros(3, 256, 256)  # Placeholder on failure\n",
    "\n",
    "    def clear_cache(self):\n",
    "        self.cache.clear()\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "# Setup the dataset and dataloader\n",
    "def get_coco_patches_loader(\n",
    "    data_dir=\"D:\\\\ds_coco_patches\", \n",
    "    batch_size=64,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    shuffle=True,\n",
    "    cache_size=0,\n",
    "    max_images=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a DataLoader for loading images from the specified directory.\n",
    "    \"\"\"\n",
    "    dataset = ImageOnlyDataset(data_dir, cache_size=cache_size, max_images=max_images)\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory\n",
    "    )\n",
    "    \n",
    "    return dataloader, dataset\n",
    "\n",
    "\n",
    "# ------------------------ Example usage ------------------------\n",
    "\n",
    "BATCH_SIZE = 32  # Adjust based on your GPU memory\n",
    "NUM_WORKERS = 0  # 0 = no multiprocessing\n",
    "MAX_IMAGES = 3000  # Limit to first 3000 images\n",
    "\n",
    "train_loader, train_dataset = get_coco_patches_loader(\n",
    "    data_dir=\"D:\\\\ds_coco_patches\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    cache_size=0,\n",
    "    max_images=MAX_IMAGES\n",
    ")\n",
    "\n",
    "# Move a batch to CUDA and measure its size\n",
    "for images in train_loader:\n",
    "    images = images.cuda()  # Move to CUDA\n",
    "    print(f\"Batch size: {images.size(0)}\")\n",
    "    print(f\"Batch shape: {images.shape}\")  # Should be [B, 3, H, W]\n",
    "    \n",
    "    # Calculate memory usage\n",
    "    batch_memory = images.element_size() * images.nelement() / (1024 ** 2)\n",
    "    print(f\"Estimated memory usage of the batch on CUDA: {batch_memory:.2f} MB\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0699def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([32, 3, 256, 256])\n",
      "Estimated batch size in MB: 24.00\n"
     ]
    }
   ],
   "source": [
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"Batch shape: {sample_batch.shape}\")\n",
    "print(f\"Estimated batch size in MB: {sample_batch.element_size() * sample_batch.nelement() / 1024**2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b1e0375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing HyperPrior on device: cuda\n",
      "DiffusionManager initialized on device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python installation\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\python installation\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from compressor.compressor import Compressor\n",
    "from decompressor.diffusion_manager import DiffusionManager\n",
    "from decompressor.unet_module import UnetModule\n",
    "\n",
    "\n",
    "# Initialize the compressor\n",
    "compressor = Compressor()\n",
    "\n",
    "# Generate a dummy tensor to simulate input data\n",
    "dummy_tensor = torch.randn(64, 3, 256, 256).cuda()\n",
    "\n",
    "# Pass the dummy tensor through the compressor\n",
    "output_dict = compressor(dummy_tensor)\n",
    "\n",
    "# Extract the shapes of the output tensors\n",
    "output_shapes = [output.shape[1] for output in output_dict['output']]\n",
    "\n",
    "# Initialize the UNet module with the extracted channel dimensions\n",
    "unet_module = UnetModule(context_channels=output_shapes)\n",
    "\n",
    "del dummy_tensor\n",
    "\n",
    "model = DiffusionManager(\n",
    "    encoder=compressor,\n",
    "    u_net=unet_module,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf296129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 15.23 MB\n"
     ]
    }
   ],
   "source": [
    "def get_model_size_in_mb(model):\n",
    "    total_size = sum(param.element_size() * param.nelement() for param in model.parameters())\n",
    "    total_size += sum(buffer.element_size() * buffer.nelement() for buffer in model.buffers())\n",
    "    return total_size / (1024 ** 2)  # Convert bytes to MB\n",
    "\n",
    "model_size_mb = get_model_size_in_mb(model)\n",
    "print(f\"Model size: {model_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40dc12ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ff6f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on cuda\n",
      "Training for 5 epochs with batch size 32\n",
      "\n",
      "Epoch 1/5\n",
      "len train_loader: 94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  19%|█▉        | 18/94 [04:00<16:19, 12.88s/it, loss=4.4239, prior_loss=3.2104]     "
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import pynvml\n",
    "pynvml.nvmlInit()\n",
    "handle = pynvml.nvmlDeviceGetHandleByIndex(torch.cuda.current_device())\n",
    "\n",
    "def print_gpu_mem(tag=\"\"):\n",
    "    info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "    allocated = torch.cuda.memory_allocated() / 1024**2\n",
    "    reserved = torch.cuda.memory_reserved() / 1024**2\n",
    "    print(f\"[{tag}] GPU Mem Used (NVML): {info.used / 1024**2:.2f} MB | Allocated (PyTorch): {allocated:.2f} MB | Reserved (PyTorch): {reserved:.2f} MB\")\n",
    "\n",
    "NUM_WORKERS = 0 if torch.cuda.is_available() else 0\n",
    "\n",
    "# Create optimizer for both compressor and UNet\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.encoder.parameters()},\n",
    "    {'params': model.u_net.parameters()}\n",
    "], lr=1e-4)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 5\n",
    "log_interval = 50  # Log every 50 batches\n",
    "save_interval = 1   # Save checkpoint every epoch\n",
    "\n",
    "# Create directory for saving checkpoints\n",
    "save_dir = os.path.join(\"checkpoints\", \n",
    "                        f\"cdc_training_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Training statistics\n",
    "train_losses = []\n",
    "prior_losses = []\n",
    "\n",
    "# Move models to CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Starting training on {device}\")\n",
    "print(f\"Training for {num_epochs} epochs with batch size {BATCH_SIZE}\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    train_dataset.clear_cache()  # Clear cache at the start of each epoch\n",
    "    epoch_losses = []\n",
    "    epoch_prior_losses = []\n",
    "\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"len train_loader: {len(train_loader)}\")\n",
    "    # print_gpu_mem(\"Epoch Start\")\n",
    "    \n",
    "    # Create tqdm progress bar\n",
    "    progress_bar = tqdm.tqdm(enumerate(train_loader), total=len(train_loader),\n",
    "                             desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    model.train()  # Set model to training mode\n",
    "    \n",
    "    for batch_idx, images in progress_bar:\n",
    "        # print_gpu_mem(\"Batch Start\")\n",
    "        images = images.to(device)  # Move images to GPU\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad(set_to_none=True) # set_to_none=True có thể tiết kiệm chút mem\n",
    "        # print_gpu_mem(\"After optimizer.zero_grad\")\n",
    "        \n",
    "        total_loss, prior_loss = model(images)\n",
    "        # print_gpu_mem(\"After model(images)\")\n",
    "        \n",
    "        total_loss.backward()\n",
    "        prior_loss.backward()\n",
    "        # print_gpu_mem(\"After total_loss.backward()\")\n",
    "\n",
    "        # --- Optimizer step ---\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        # print_gpu_mem(\"After clip_grad_norm_\")\n",
    "        optimizer.step()\n",
    "        # print_gpu_mem(\"After optimizer.step()\")\n",
    "        \n",
    "        loss_item = total_loss.item() # Lấy item() NGAY LẬP TỨC\n",
    "        prior_loss_item = prior_loss.item()\n",
    "        epoch_losses.append(loss_item)\n",
    "        epoch_prior_losses.append(prior_loss_item)\n",
    "        # print_gpu_mem(\"After loss .item()\")\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f\"{epoch_losses[-1]:.4f}\",\n",
    "            'prior_loss': f\"{epoch_prior_losses[-1]:.4f}\"\n",
    "        })\n",
    "        \n",
    "        # Log periodically\n",
    "        if batch_idx % log_interval == 0 and batch_idx > 0:\n",
    "            print(f\"\\nBatch {batch_idx}/{len(train_loader)}, \"\n",
    "                  f\"Loss: {sum(epoch_losses[-log_interval:]) / log_interval:.4f}, \"\n",
    "                  f\"Prior Loss: {sum(epoch_prior_losses[-log_interval:]) / log_interval:.4f}\")\n",
    "            \n",
    "        time.sleep(5)\n",
    "\n",
    "        del images, total_loss, prior_loss, loss_item, prior_loss_item\n",
    "        # print_gpu_mem(\"Batch End (after explicit del)\")\n",
    "    \n",
    "    # Calculate average epoch loss\n",
    "    avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "    avg_prior_loss = sum(epoch_prior_losses) / len(epoch_prior_losses)\n",
    "    train_losses.append(avg_loss)\n",
    "    prior_losses.append(avg_prior_loss)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs} completed, \"\n",
    "          f\"Avg Loss: {avg_loss:.4f}, \"\n",
    "          f\"Avg Prior Loss: {avg_prior_loss:.4f}\")\n",
    "    \n",
    "    # Save checkpoints\n",
    "    if (epoch + 1) % save_interval == 0:\n",
    "        checkpoint_path = os.path.join(save_dir, f\"checkpoint_epoch_{epoch+1}.pt\")\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'encoder_state_dict': model.encoder.state_dict(),\n",
    "            'unet_state_dict': model.u_net.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Saved checkpoint to {checkpoint_path}\")\n",
    "\n",
    "# Plot training curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Total Loss')\n",
    "plt.plot(range(1, num_epochs + 1), prior_losses, label='Prior Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
