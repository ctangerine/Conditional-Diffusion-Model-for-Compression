{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22278f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "# sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), './')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9a101c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "COCO_ROOT_DIR = r\"D:\\temp_dataset\\coco\\images\\train2017.1\\train2017\"  # Thư mục gốc chứa COCO\n",
    "TRAIN_IMAGES_SUBDIR = \"\"     # Thư mục con chứa ảnh train2017\n",
    "TRAIN_ANNOTATIONS_FILENAME = \"captions_train2017.json\" # File chú thích cho train2017\n",
    "\n",
    "PATCH_SIZE = (256, 256)  # Kích thước patch mong muốn (height, width)\n",
    "BATCH_SIZE = 32          # Kích thước batch cho DataLoader\n",
    "NUM_WORKERS = 4         # Số luồng để tải dữ liệu\n",
    "\n",
    "# --- Xây dựng đường dẫn đầy đủ ---\n",
    "train_images_path = os.path.join(COCO_ROOT_DIR, TRAIN_IMAGES_SUBDIR)\n",
    "train_annotations_path = os.path.join(r\"D:\\temp_dataset\\coco\\images\\train2017.1\\annotations_trainval2017\\annotations\", TRAIN_ANNOTATIONS_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a3cc2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 images in D:\\ds_coco_patches\n",
      "Batch size: 1\n",
      "Batch shape: torch.Size([1, 3, 256, 256])\n",
      "Estimated memory usage of the batch on CUDA: 0.75 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ------------------------ Example usage ------------------------\n",
    "\n",
    "from data_loader import get_coco_patches_loader\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32  # Adjust based on your GPU memory\n",
    "NUM_WORKERS = 4  # 0 = no multiprocessing\n",
    "MAX_IMAGES = 1  # Limit to first 3000 images\n",
    "\n",
    "train_loader, train_dataset = get_coco_patches_loader(\n",
    "    data_dir=\"D:\\\\ds_coco_patches\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    pin_memory=True,\n",
    "    num_workers=0 if torch.cuda.is_available() else 0,\n",
    "    shuffle=True,\n",
    "    cache_size=0,\n",
    "    max_images=MAX_IMAGES\n",
    ")\n",
    "\n",
    "# Move a batch to CUDA and measure its size\n",
    "for images in train_loader:\n",
    "    images = images.cuda()  # Move to CUDA\n",
    "    print(f\"Batch size: {images.size(0)}\")\n",
    "    print(f\"Batch shape: {images.shape}\")  # Should be [B, 3, H, W]\n",
    "    \n",
    "    # Calculate memory usage\n",
    "    batch_memory = images.element_size() * images.nelement() / (1024 ** 2)\n",
    "    print(f\"Estimated memory usage of the batch on CUDA: {batch_memory:.2f} MB\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0699def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([1, 3, 256, 256])\n",
      "Estimated batch size in MB: 0.75\n"
     ]
    }
   ],
   "source": [
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"Batch shape: {sample_batch.shape}\")\n",
    "print(f\"Estimated batch size in MB: {sample_batch.element_size() * sample_batch.nelement() / 1024**2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ccad66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet Encoder Input-Output Pairs: [(3, 32), (32, 64), (64, 96), (96, 128), (128, 160), (160, 192), (192, 224), (224, 256)]\n",
      "UNet Encoder Channels Pairs: [(6, 32), (64, 64), (128, 96), (192, 128), (256, 160), (320, 192), (192, 224), (224, 256)]\n",
      "UNet Decoder Channels Pairs: [(512, 224), (448, 192), (384, 160), (320, 128), (256, 96), (192, 64), (128, 32), (64, 3)]\n",
      "Context Channels: [3, 32, 64, 96, 128, 160]\n"
     ]
    }
   ],
   "source": [
    "# from cdc_trainable import CDCTrainable\n",
    "from cdc_trainable import CDCTrainable\n",
    "from decompressor.diffusion_manager import DiffusionManager\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CDCTrainable(device=device)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b1e0375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from compressor.compressor import Compressor\n",
    "# from decompressor.diffusion_manager import DiffusionManager\n",
    "# from decompressor.unet_module import UnetModule\n",
    "\n",
    "\n",
    "# # Initialize the compressor\n",
    "# compressor = Compressor(\n",
    "#     channel_multiplier=[1, 3, 3, 12, 52, 64],\n",
    "#     hyperprior_channel_multiplier=[64,64,64]\n",
    "# )\n",
    "\n",
    "# # Generate a dummy tensor to simulate input data\n",
    "# dummy_tensor = torch.randn(64, 3, 256, 256).cuda()\n",
    "\n",
    "# # Pass the dummy tensor through the compressor\n",
    "# output_dict = compressor(dummy_tensor)\n",
    "\n",
    "# # Extract the shapes of the output tensors\n",
    "# output_shapes = [output.shape[1] for output in output_dict['output']]\n",
    "\n",
    "# # Initialize the UNet module with the extracted channel dimensions\n",
    "# unet_module = UnetModule(\n",
    "#     base_channels=3,\n",
    "#     context_channels=output_shapes\n",
    "# )\n",
    "\n",
    "# del dummy_tensor\n",
    "\n",
    "# model = DiffusionManager(\n",
    "#     encoder=compressor,\n",
    "#     u_net=unet_module,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf296129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 187.31 MB\n"
     ]
    }
   ],
   "source": [
    "def get_model_size_in_mb(model):\n",
    "    total_size = sum(param.element_size() * param.nelement() for param in model.parameters())\n",
    "    total_size += sum(buffer.element_size() * buffer.nelement() for buffer in model.buffers())\n",
    "    return total_size / (1024 ** 2)  # Convert bytes to MB\n",
    "\n",
    "model_size_mb = get_model_size_in_mb(model)\n",
    "print(f\"Model size: {model_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40dc12ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2241059a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_image(input, save_dir=\"visualizations\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    image = input[0].cpu().detach().numpy()\n",
    "    image = np.transpose(image, (1, 2, 0))\n",
    "    image = (image * 255).astype(np.uint8)\n",
    "    image = Image.fromarray(image)\n",
    "    # Generate a unique filename using timestamp\n",
    "    filename = f\"image_{int(time.time() * 1000)}.png\"\n",
    "    save_path = os.path.join(save_dir, filename)\n",
    "    image.save(save_path)\n",
    "    # print(f\"Image saved to {save_path}\")\n",
    "    image.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9b51fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "from torch.amp import autocast, GradScaler\n",
    "import tqdm \n",
    "import datetime\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "NUM_WORKERS = 0 if not torch.cuda.is_available() else 0 \n",
    "\n",
    "num_epochs = 20000\n",
    "log_interval = 100\n",
    "save_interval = 1000\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=1e-7,  # Typical for diffusion models\n",
    "    weight_decay=1e-4,\n",
    "    betas=(0.9, 0.999)\n",
    ")\n",
    "\n",
    "# Cosine annealing scheduler is common for diffusion models\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=num_epochs,\n",
    "    eta_min=1e-5\n",
    ")\n",
    "\n",
    "save_dir = os.path.join(\"checkpoints\", \n",
    "                        f\"cdc_training_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "train_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cd325df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint from E:\\DUT Courses\\Academic year 4\\semester 2\\PBL\\PBL7 CDC Compression\\checkpoints\\cdc_training_20250603_212143\\checkpoint_epoch_20000.pt\n"
     ]
    }
   ],
   "source": [
    "save_dir = r\"E:\\DUT Courses\\Academic year 4\\semester 2\\PBL\\PBL7 CDC Compression\\checkpoints\\cdc_training_20250603_212143\"\n",
    "\n",
    "# Load checkpoint \n",
    "checkpoint_path = os.path.join(save_dir, f\"checkpoint_epoch_{20000}.pt\")\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "epochs = checkpoint['epoch']\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "model.train()  # Set model to training mode\n",
    "print(f\"Loaded checkpoint from {checkpoint_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2de3a6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20001/20000: 100%|██████████| 1/1 [00:02<00:00,  2.72s/it, loss=0.5016]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20001 completed - Avg Loss: 0.5016\n",
      "DDIM Step: 50/50\n",
      "DDIM Step: 49/50\n",
      "DDIM Step: 48/50\n",
      "DDIM Step: 47/50\n",
      "DDIM Step: 46/50\n",
      "DDIM Step: 45/50\n",
      "DDIM Step: 44/50\n",
      "DDIM Step: 43/50\n",
      "DDIM Step: 42/50\n",
      "DDIM Step: 41/50\n",
      "DDIM Step: 40/50\n",
      "DDIM Step: 39/50\n",
      "DDIM Step: 38/50\n",
      "DDIM Step: 37/50\n",
      "DDIM Step: 36/50\n",
      "DDIM Step: 35/50\n",
      "DDIM Step: 34/50\n",
      "DDIM Step: 33/50\n",
      "DDIM Step: 32/50\n",
      "DDIM Step: 31/50\n",
      "DDIM Step: 30/50\n",
      "DDIM Step: 29/50\n",
      "DDIM Step: 28/50\n",
      "DDIM Step: 27/50\n",
      "DDIM Step: 26/50\n",
      "DDIM Step: 25/50\n",
      "DDIM Step: 24/50\n",
      "DDIM Step: 23/50\n",
      "DDIM Step: 22/50\n",
      "DDIM Step: 21/50\n",
      "DDIM Step: 20/50\n",
      "DDIM Step: 19/50\n",
      "DDIM Step: 18/50\n",
      "DDIM Step: 17/50\n",
      "DDIM Step: 16/50\n",
      "DDIM Step: 15/50\n",
      "DDIM Step: 14/50\n",
      "DDIM Step: 13/50\n",
      "DDIM Step: 12/50\n",
      "DDIM Step: 11/50\n",
      "DDIM Step: 10/50\n",
      "DDIM Step: 9/50\n",
      "DDIM Step: 8/50\n",
      "DDIM Step: 7/50\n",
      "DDIM Step: 6/50\n",
      "DDIM Step: 5/50\n",
      "DDIM Step: 4/50\n",
      "DDIM Step: 3/50\n",
      "DDIM Step: 2/50\n",
      "DDIM Step: 1/50\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs, num_epochs+60):\n",
    "    train_dataset.clear_cache()\n",
    "    epoch_losses = []\n",
    "    model.train()\n",
    "\n",
    "    progress_bar = tqdm.tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    image_for_viz = None\n",
    "    predicted_x0 = None\n",
    "    test_image = None\n",
    "\n",
    "    for batch_idx, images in progress_bar:\n",
    "        images = images.to(device)\n",
    "        test_image = images[0].unsqueeze(0)  # For visualization purposes\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with autocast(device_type='cuda', enabled=torch.cuda.is_available()):\n",
    "            loss, x0_img, image_noise = model(images)\n",
    "\n",
    "        if batch_idx == 0:\n",
    "            predicted_x0 = x0_img\n",
    "            image_for_viz = image_noise[0].unsqueeze(0)\n",
    "\n",
    "        if torch.isnan(loss).any():\n",
    "            print(f\"NaN in loss at batch {batch_idx}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        if any(torch.isnan(param.grad).any() for param in model.parameters() if param.grad is not None):\n",
    "            print(f\"NaN in gradients at batch {batch_idx}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_losses.append(loss.item())\n",
    "\n",
    "        progress_bar.set_postfix({'loss': f\"{epoch_losses[-1]:.4f}\"})\n",
    "\n",
    "        if batch_idx % log_interval == 0 and batch_idx > 0:\n",
    "            avg_recent_loss = sum(epoch_losses[-log_interval:]) / log_interval\n",
    "            print(f\"Batch {batch_idx}/{len(train_loader)} - Avg Loss: {avg_recent_loss:.4f}\")\n",
    "\n",
    "        del loss\n",
    "\n",
    "    avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    print(f\"\\nEpoch {epoch+1} completed - Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % save_interval == 0:\n",
    "        checkpoint_path = os.path.join(save_dir, f\"checkpoint_epoch_{epoch+1}.pt\")\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "    # if (epoch) >= 10000 and predicted_x0 is not None and (epoch + 1) % 500 == 0:\n",
    "    draw_image(predicted_x0)\n",
    "    draw_image(image_for_viz)\n",
    "\n",
    "    if (epoch) >= 1 and (epoch + 1) % 1 == 0:\n",
    "        model.eval()\n",
    "        # draw_image(test_image, save_dir=\"visualizations_test\")\n",
    "        model.ddim_forward(test_image, denoise_steps=50)\n",
    "        model.train()\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        break\n",
    "\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ea504a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "\n",
    "# images = next(iter(train_loader))\n",
    "\n",
    "# for epoch in range(0, 50):\n",
    "#     train_dataset.clear_cache()\n",
    "#     epoch_losses = []\n",
    "\n",
    "#     progress_bar = tqdm.tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "#     image_for_viz = None\n",
    "#     predicted_x0 = None\n",
    "#     test_image = None\n",
    "\n",
    "\n",
    "#     images = images.to(device)\n",
    "#     test_image = images[0].unsqueeze(0)  # For visualization purposes\n",
    "\n",
    "#     optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "#     with autocast(device_type='cuda', enabled=torch.cuda.is_available()):\n",
    "#         loss, x0_img, image_noise = model(images)\n",
    "\n",
    "#     if True:\n",
    "#         predicted_x0 = x0_img\n",
    "#         image_for_viz = image_noise[0].unsqueeze(0)\n",
    "\n",
    "#     # if (epoch) >= 10000 and predicted_x0 is not None and (epoch + 1) % 500 == 0:\n",
    "#     draw_image(predicted_x0)\n",
    "#     draw_image(image_for_viz)\n",
    "\n",
    "#     images = predicted_x0\n",
    "\n",
    "#     if (epoch) >= 1000 and (epoch + 1) % 100 == 0:\n",
    "#         model.eval()\n",
    "#         # draw_image(test_image, save_dir=\"visualizations_test\")\n",
    "#         model.ddim_forward(test_image, denoise_steps=50)\n",
    "#         model.train()\n",
    "\n",
    "#         torch.cuda.empty_cache()\n",
    "#         gc.collect()\n",
    "#         # break\n",
    "\n",
    "#     scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e47f744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model saved for evaluation at E:\\DUT Courses\\Academic year 4\\semester 2\\PBL\\PBL7 CDC Compression\\checkpoints\\cdc_training_20250603_212143\\final_model.pt\n"
     ]
    }
   ],
   "source": [
    "# Save model as final (for evaluation/inference)\n",
    "model.eval()\n",
    "final_checkpoint_path = os.path.join(save_dir, \"final_model.pt\")\n",
    "torch.save({\n",
    "    'epoch': num_epochs,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "}, final_checkpoint_path)\n",
    "print(f\"Final model saved for evaluation at {final_checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0e5190",
   "metadata": {},
   "source": [
    "<h2>OLD MODEL TRAINING PHASE</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb3ced4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CDCTrainable(\n",
      "  (extractor): ModuleList(\n",
      "    (0): ModuleList(\n",
      "      (0): ResnetBlock(\n",
      "        (block1): Sequential(\n",
      "          (0): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (block2): Sequential(\n",
      "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (shortcut): Conv2d(3, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (1): DownSampling(\n",
      "        (conv_layer): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (1): ModuleList(\n",
      "      (0): ResnetBlock(\n",
      "        (block1): Sequential(\n",
      "          (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (block2): Sequential(\n",
      "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (shortcut): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (1): DownSampling(\n",
      "        (conv_layer): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (2): ModuleList(\n",
      "      (0): ResnetBlock(\n",
      "        (block1): Sequential(\n",
      "          (0): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (block2): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (shortcut): Conv2d(64, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (1): DownSampling(\n",
      "        (conv_layer): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (3): ModuleList(\n",
      "      (0): ResnetBlock(\n",
      "        (block1): Sequential(\n",
      "          (0): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (block2): Sequential(\n",
      "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (shortcut): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (1): DownSampling(\n",
      "        (conv_layer): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (4): ModuleList(\n",
      "      (0): ResnetBlock(\n",
      "        (block1): Sequential(\n",
      "          (0): Conv2d(128, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (block2): Sequential(\n",
      "          (0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (shortcut): Conv2d(128, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (1): DownSampling(\n",
      "        (conv_layer): Conv2d(160, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (5): ModuleList(\n",
      "      (0): ResnetBlock(\n",
      "        (block1): Sequential(\n",
      "          (0): Conv2d(160, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (block2): Sequential(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (shortcut): Conv2d(160, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (1): DownSampling(\n",
      "        (conv_layer): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (hyper_encoder): ModuleList(\n",
      "    (0): ModuleList(\n",
      "      (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "      (1): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "    (1): ModuleList(\n",
      "      (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "      (1): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "    (2): ModuleList(\n",
      "      (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "      (1): Identity()\n",
      "    )\n",
      "  )\n",
      "  (hyper_decoder): ModuleList(\n",
      "    (0-1): 2 x ModuleList(\n",
      "      (0): ConvTranspose2d(192, 192, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
      "      (1): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "    (2): ModuleList(\n",
      "      (0): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): Identity()\n",
      "    )\n",
      "  )\n",
      "  (feature_generator): ModuleList(\n",
      "    (0): ModuleList(\n",
      "      (0): ResnetBlock(\n",
      "        (block1): Sequential(\n",
      "          (0): Conv2d(192, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (block2): Sequential(\n",
      "          (0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (shortcut): Conv2d(192, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (1): UpSampling(\n",
      "        (conv_layer): ConvTranspose2d(160, 160, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (1): ModuleList(\n",
      "      (0): ResnetBlock(\n",
      "        (block1): Sequential(\n",
      "          (0): Conv2d(160, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (block2): Sequential(\n",
      "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (shortcut): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (1): UpSampling(\n",
      "        (conv_layer): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (2): ModuleList(\n",
      "      (0): ResnetBlock(\n",
      "        (block1): Sequential(\n",
      "          (0): Conv2d(128, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (block2): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (shortcut): Conv2d(128, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (1): UpSampling(\n",
      "        (conv_layer): ConvTranspose2d(96, 96, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (3): ModuleList(\n",
      "      (0): ResnetBlock(\n",
      "        (block1): Sequential(\n",
      "          (0): Conv2d(96, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (block2): Sequential(\n",
      "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (shortcut): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (1): UpSampling(\n",
      "        (conv_layer): ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (4): ModuleList(\n",
      "      (0): ResnetBlock(\n",
      "        (block1): Sequential(\n",
      "          (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (block2): Sequential(\n",
      "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (shortcut): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (1): UpSampling(\n",
      "        (conv_layer): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (5): ModuleList(\n",
      "      (0): ResnetBlock(\n",
      "        (block1): Sequential(\n",
      "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (block2): Sequential(\n",
      "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "      )\n",
      "      (1): UpSampling(\n",
      "        (conv_layer): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (image_generator_unet): ModuleList()\n",
      "  (unet_encoder): ModuleList(\n",
      "    (0): ModuleList(\n",
      "      (0): ResnetBlock(\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=6, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          (2): Linear(in_features=6, out_features=6, bias=True)\n",
      "        )\n",
      "        (block1): Sequential(\n",
      "          (0): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (block2): Sequential(\n",
      "          (0): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "      )\n",
      "      (1): ResnetBlock(\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=32, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (block1): Sequential(\n",
      "          (0): Conv2d(6, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (block2): Sequential(\n",
      "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (shortcut): Conv2d(6, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (2): LayerNorm()\n",
      "      (3): LinearAttention(\n",
      "        (to_QueryKeyValue): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (to_output): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (4): DownSampling(\n",
      "        (conv_layer): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (1): ModuleList(\n",
      "      (0-1): 2 x ResnetBlock(\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=64, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (block1): Sequential(\n",
      "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (block2): Sequential(\n",
      "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "      )\n",
      "      (2): LayerNorm()\n",
      "      (3): LinearAttention(\n",
      "        (to_QueryKeyValue): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (to_output): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (4): DownSampling(\n",
      "        (conv_layer): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (2): ModuleList(\n",
      "      (0): ResnetBlock(\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (block1): Sequential(\n",
      "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (block2): Sequential(\n",
      "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "      )\n",
      "      (1): ResnetBlock(\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=96, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          (2): Linear(in_features=96, out_features=96, bias=True)\n",
      "        )\n",
      "        (block1): Sequential(\n",
      "          (0): Conv2d(128, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (block2): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (shortcut): Conv2d(128, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (2): LayerNorm()\n",
      "      (3): LinearAttention(\n",
      "        (to_QueryKeyValue): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (to_output): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (4): DownSampling(\n",
      "        (conv_layer): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (3): ModuleList(\n",
      "      (0): ResnetBlock(\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=192, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          (2): Linear(in_features=192, out_features=192, bias=True)\n",
      "        )\n",
      "        (block1): Sequential(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (block2): Sequential(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "      )\n",
      "      (1): ResnetBlock(\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (block1): Sequential(\n",
      "          (0): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (block2): Sequential(\n",
      "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (shortcut): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (2): LayerNorm()\n",
      "      (3): LinearAttention(\n",
      "        (to_QueryKeyValue): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (to_output): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (4): DownSampling(\n",
      "        (conv_layer): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (4): ModuleList(\n",
      "      (0): ResnetBlock(\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (block1): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (block2): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "      )\n",
      "      (1): ResnetBlock(\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=160, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          (2): Linear(in_features=160, out_features=160, bias=True)\n",
      "        )\n",
      "        (block1): Sequential(\n",
      "          (0): Conv2d(256, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (block2): Sequential(\n",
      "          (0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (shortcut): Conv2d(256, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (2): LayerNorm()\n",
      "      (3): LinearAttention(\n",
      "        (to_QueryKeyValue): Conv2d(160, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (to_output): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (4): DownSampling(\n",
      "        (conv_layer): Conv2d(160, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (5): ModuleList(\n",
      "      (0): ResnetBlock(\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=320, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          (2): Linear(in_features=320, out_features=320, bias=True)\n",
      "        )\n",
      "        (block1): Sequential(\n",
      "          (0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (block2): Sequential(\n",
      "          (0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "      )\n",
      "      (1): ResnetBlock(\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=192, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          (2): Linear(in_features=192, out_features=192, bias=True)\n",
      "        )\n",
      "        (block1): Sequential(\n",
      "          (0): Conv2d(320, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (block2): Sequential(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (shortcut): Conv2d(320, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (2): LayerNorm()\n",
      "      (3): LinearAttention(\n",
      "        (to_QueryKeyValue): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (to_output): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (4): DownSampling(\n",
      "        (conv_layer): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (6): ModuleList(\n",
      "      (0): ResnetBlock(\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=192, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          (2): Linear(in_features=192, out_features=192, bias=True)\n",
      "        )\n",
      "        (block1): Sequential(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (block2): Sequential(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "      )\n",
      "      (1): ResnetBlock(\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=224, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          (2): Linear(in_features=224, out_features=224, bias=True)\n",
      "        )\n",
      "        (block1): Sequential(\n",
      "          (0): Conv2d(192, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (block2): Sequential(\n",
      "          (0): Conv2d(224, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (shortcut): Conv2d(192, 224, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (2): LayerNorm()\n",
      "      (3): LinearAttention(\n",
      "        (to_QueryKeyValue): Conv2d(224, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (to_output): Conv2d(224, 224, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (4): DownSampling(\n",
      "        (conv_layer): Conv2d(224, 224, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (7): ModuleList(\n",
      "      (0): ResnetBlock(\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=224, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          (2): Linear(in_features=224, out_features=224, bias=True)\n",
      "        )\n",
      "        (block1): Sequential(\n",
      "          (0): Conv2d(224, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (block2): Sequential(\n",
      "          (0): Conv2d(224, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "      )\n",
      "      (1): ResnetBlock(\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (block1): Sequential(\n",
      "          (0): Conv2d(224, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (block2): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (shortcut): Conv2d(224, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (2): LayerNorm()\n",
      "      (3): LinearAttention(\n",
      "        (to_QueryKeyValue): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (to_output): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (4): Identity()\n",
      "    )\n",
      "  )\n",
      "  (unet_bottleneck): ModuleList(\n",
      "    (0): ModuleList(\n",
      "      (0): ResnetBlock(\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (block1): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (block2): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "      )\n",
      "      (1): LayerNorm()\n",
      "      (2): LinearAttention(\n",
      "        (to_QueryKeyValue): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (to_output): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (3): ResnetBlock(\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (block1): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (block2): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (unet_decoder): ModuleList(\n",
      "    (0): ModuleList(\n",
      "      (0): ResnetBlock(\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (block1): Sequential(\n",
      "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (block2): Sequential(\n",
      "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "      )\n",
      "      (1): ResnetBlock(\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=224, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          (2): Linear(in_features=224, out_features=224, bias=True)\n",
      "        )\n",
      "        (block1): Sequential(\n",
      "          (0): Conv2d(512, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (block2): Sequential(\n",
      "          (0): Conv2d(224, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (shortcut): Conv2d(512, 224, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (2): LayerNorm()\n",
      "      (3): LinearAttention(\n",
      "        (to_QueryKeyValue): Conv2d(224, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (to_output): Conv2d(224, 224, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (4): UpSampling(\n",
      "        (conv_layer): ConvTranspose2d(224, 224, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (1): ModuleList(\n",
      "      (0): ResnetBlock(\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=448, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          (2): Linear(in_features=448, out_features=448, bias=True)\n",
      "        )\n",
      "        (block1): Sequential(\n",
      "          (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (block2): Sequential(\n",
      "          (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "      )\n",
      "      (1): ResnetBlock(\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=192, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          (2): Linear(in_features=192, out_features=192, bias=True)\n",
      "        )\n",
      "        (block1): Sequential(\n",
      "          (0): Conv2d(448, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (block2): Sequential(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (shortcut): Conv2d(448, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (2): LayerNorm()\n",
      "      (3): LinearAttention(\n",
      "        (to_QueryKeyValue): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (to_output): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (4): UpSampling(\n",
      "        (conv_layer): ConvTranspose2d(192, 192, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (2): ModuleList(\n",
      "      (0): ResnetBlock(\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=384, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          (2): Linear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (block1): Sequential(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (block2): Sequential(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "      )\n",
      "      (1): ResnetBlock(\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=160, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          (2): Linear(in_features=160, out_features=160, bias=True)\n",
      "        )\n",
      "        (block1): Sequential(\n",
      "          (0): Conv2d(384, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (block2): Sequential(\n",
      "          (0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (shortcut): Conv2d(384, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (2): LayerNorm()\n",
      "      (3): LinearAttention(\n",
      "        (to_QueryKeyValue): Conv2d(160, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (to_output): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (4): UpSampling(\n",
      "        (conv_layer): ConvTranspose2d(160, 160, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (3): ModuleList(\n",
      "      (0): ResnetBlock(\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=320, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          (2): Linear(in_features=320, out_features=320, bias=True)\n",
      "        )\n",
      "        (block1): Sequential(\n",
      "          (0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (block2): Sequential(\n",
      "          (0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "      )\n",
      "      (1): ResnetBlock(\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (block1): Sequential(\n",
      "          (0): Conv2d(320, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (block2): Sequential(\n",
      "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (shortcut): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (2): LayerNorm()\n",
      "      (3): LinearAttention(\n",
      "        (to_QueryKeyValue): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (to_output): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (4): UpSampling(\n",
      "        (conv_layer): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (4): ModuleList(\n",
      "      (0): ResnetBlock(\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (block1): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (block2): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "      )\n",
      "      (1): ResnetBlock(\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=96, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          (2): Linear(in_features=96, out_features=96, bias=True)\n",
      "        )\n",
      "        (block1): Sequential(\n",
      "          (0): Conv2d(256, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (block2): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (shortcut): Conv2d(256, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (2): LayerNorm()\n",
      "      (3): LinearAttention(\n",
      "        (to_QueryKeyValue): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (to_output): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (4): UpSampling(\n",
      "        (conv_layer): ConvTranspose2d(96, 96, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (5): ModuleList(\n",
      "      (0): ResnetBlock(\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=192, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          (2): Linear(in_features=192, out_features=192, bias=True)\n",
      "        )\n",
      "        (block1): Sequential(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (block2): Sequential(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "      )\n",
      "      (1): ResnetBlock(\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=64, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (block1): Sequential(\n",
      "          (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (block2): Sequential(\n",
      "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (shortcut): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (2): LayerNorm()\n",
      "      (3): LinearAttention(\n",
      "        (to_QueryKeyValue): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (to_output): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (4): UpSampling(\n",
      "        (conv_layer): ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (6): ModuleList(\n",
      "      (0): ResnetBlock(\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (block1): Sequential(\n",
      "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (block2): Sequential(\n",
      "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "      )\n",
      "      (1): ResnetBlock(\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=32, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (block1): Sequential(\n",
      "          (0): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (block2): Sequential(\n",
      "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (shortcut): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (2): LayerNorm()\n",
      "      (3): LinearAttention(\n",
      "        (to_QueryKeyValue): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (to_output): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (4): UpSampling(\n",
      "        (conv_layer): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (7): ModuleList(\n",
      "      (0): ResnetBlock(\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=64, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (block1): Sequential(\n",
      "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (block2): Sequential(\n",
      "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "      )\n",
      "      (1): ResnetBlock(\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=3, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          (2): Linear(in_features=3, out_features=3, bias=True)\n",
      "        )\n",
      "        (block1): Sequential(\n",
      "          (0): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (block2): Sequential(\n",
      "          (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LayerNorm()\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (shortcut): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (2): LayerNorm()\n",
      "      (3): LinearAttention(\n",
      "        (to_QueryKeyValue): Conv2d(3, 9, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (to_output): Conv2d(3, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (4): Identity()\n",
      "    )\n",
      "    (8): Sequential(\n",
      "      (0): LayerNorm()\n",
      "      (1): Conv2d(3, 3, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "    )\n",
      "  )\n",
      "  (time_embedding): Sequential(\n",
      "    (0): Linear(in_features=1, out_features=64, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): Linear(in_features=64, out_features=256, bias=True)\n",
      "    (3): SiLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2885888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_dir = r\"E:\\DUT Courses\\Academic year 4\\semester 2\\PBL\\PBL7 CDC Compression\\checkpoints\\cdc_training_20250601_105319\"\n",
    "\n",
    "# # Load checkpoint \n",
    "# checkpoint_path = os.path.join(save_dir, f\"checkpoint_epoch_{1000}.pt\")\n",
    "# checkpoint = torch.load(checkpoint_path)\n",
    "# model.encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "# model.u_net.load_state_dict(checkpoint['unet_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# model.train()  # Set model to training mode\n",
    "# print(f\"Loaded checkpoint from {checkpoint_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3ff6f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.autograd.set_detect_anomaly(True)\n",
    "# from torch.utils.data import DataLoader\n",
    "# import tqdm\n",
    "# import datetime\n",
    "# import os\n",
    "# from PIL import Image\n",
    "\n",
    "# import pynvml\n",
    "# pynvml.nvmlInit()\n",
    "# handle = pynvml.nvmlDeviceGetHandleByIndex(torch.cuda.current_device())\n",
    "\n",
    "# def print_gpu_mem(tag=\"\"):\n",
    "#     info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "#     allocated = torch.cuda.memory_allocated() / 1024**2\n",
    "#     reserved = torch.cuda.memory_reserved() / 1024**2\n",
    "#     print(f\"[{tag}] GPU Mem Used (NVML): {info.used / 1024**2:.2f} MB | Allocated (PyTorch): {allocated:.2f} MB | Reserved (PyTorch): {reserved:.2f} MB\")\n",
    "\n",
    "# NUM_WORKERS = 0 if torch.cuda.is_available() else 0\n",
    "\n",
    "# # Training parameters\n",
    "# num_epochs = 20000\n",
    "# log_interval = 50  # Log every 50 batches\n",
    "# save_interval = 100   # Save checkpoint every epoch\n",
    "\n",
    "# # Create optimizer for both compressor and UNet - REMOVED mixed precision components\n",
    "# optimizer = torch.optim.AdamW(\n",
    "#     model.parameters(),\n",
    "#     lr=2e-3,  # Typical for diffusion models\n",
    "#     weight_decay=1e-3,\n",
    "#     betas=(0.9, 0.999)\n",
    "# )\n",
    "\n",
    "# # Cosine annealing scheduler is common for diffusion models\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "#     optimizer,\n",
    "#     T_max=num_epochs,\n",
    "#     eta_min=1e-5\n",
    "# )\n",
    "\n",
    "# # Create directory for saving checkpoints\n",
    "# save_dir = os.path.join(\"checkpoints\", \n",
    "#                         f\"cdc_training_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "# os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# # Training statistics\n",
    "# train_losses = []\n",
    "# prior_losses = []\n",
    "\n",
    "# # Move models to CUDA\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = model.to(device)\n",
    "\n",
    "# print(f\"Starting training on {device}\")\n",
    "# print(f\"Training for {num_epochs} epochs with batch size {BATCH_SIZE}\")\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in range(num_epochs):\n",
    "#     train_dataset.clear_cache()  # Clear cache at the start of each epoch\n",
    "#     epoch_losses = []\n",
    "#     epoch_prior_losses = []\n",
    "    \n",
    "#     # Create tqdm progress bar\n",
    "#     progress_bar = tqdm.tqdm(enumerate(train_loader), total=len(train_loader),\n",
    "#                             desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "#     model.train()  # Set model to training mode\n",
    "#     predict_x0_viz = None\n",
    "#     noise = None\n",
    "\n",
    "#     sav_images = None\n",
    "    \n",
    "#     for batch_idx, images in progress_bar:\n",
    "#         images = images.to(device)  # Move images to GPU\n",
    "#         sav_images = images\n",
    "#         # Zero gradients\n",
    "#         optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "#         # Normal forward pass without autocast\n",
    "#         total_loss, prior_loss, estimate_x0, loss_dict, noise_add = model(images)\n",
    "#         # Save a batch for visualization at end of epoch\n",
    "#         if batch_idx == 0:\n",
    "#             predict_x0_viz = estimate_x0    \n",
    "#             noise = noise_add\n",
    "        \n",
    "#         # Check for NaN in losses\n",
    "#         if torch.isnan(total_loss).any() or torch.isnan(prior_loss).any():\n",
    "#             print(f\"NaN detected in losses at batch {batch_idx}. Skipping this batch.\")\n",
    "#             continue\n",
    "\n",
    "#         # combined_loss = total_loss + prior_loss\n",
    "\n",
    "#         # # Check for NaN in loss\n",
    "#         # if torch.isnan(combined_loss).any():\n",
    "#         #     print(f\"NaN detected in combined_loss at batch {batch_idx}. Skipping this batch.\")\n",
    "#         #     continue\n",
    "\n",
    "#         # Standard backward pass\n",
    "#         # combined_loss.backward()\n",
    "#         total_loss.backward()\n",
    "#         # prior_loss.backward()\n",
    "\n",
    "#         # Clip gradients to prevent exploding gradients\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "#         # Check for NaN in gradients\n",
    "#         has_nan_grad = False\n",
    "#         for name, param in model.named_parameters():\n",
    "#             if param.grad is not None and torch.isnan(param.grad).any():\n",
    "#                 print(f\"NaN detected in gradients of parameter {name}. Skipping this batch.\")\n",
    "#                 has_nan_grad = True\n",
    "#                 break\n",
    "        \n",
    "#         if has_nan_grad:\n",
    "#             continue\n",
    "\n",
    "#         # Standard optimizer step\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         loss_item = total_loss.item()\n",
    "#         prior_loss_item = prior_loss.item()\n",
    "#         epoch_losses.append(loss_item)\n",
    "#         epoch_prior_losses.append(prior_loss_item)\n",
    "        \n",
    "#         # Update progress bar\n",
    "#         progress_bar.set_postfix({\n",
    "#             'loss': f\"{epoch_losses[-1]:.4f}\",\n",
    "#             'prior_loss': f\"{epoch_prior_losses[-1]:.4f}\",\n",
    "#             **{key: f\"{value:.4f}\" for key, value in loss_dict.items()}\n",
    "#         })\n",
    "        \n",
    "#         # Log periodically\n",
    "#         if batch_idx % log_interval == 0 and batch_idx > 0:\n",
    "#             print(f\"\\nBatch {batch_idx}/{len(train_loader)}, \"\n",
    "#                 f\"Loss: {sum(epoch_losses[-log_interval:]) / log_interval:.4f}, \"\n",
    "#                 f\"Prior Loss: {sum(epoch_prior_losses[-log_interval:]) / log_interval:.4f}\")\n",
    "\n",
    "#         del total_loss, prior_loss, loss_item, prior_loss_item\n",
    "    \n",
    "#     # Calculate average epoch loss\n",
    "#     avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "#     avg_prior_loss = sum(epoch_prior_losses) / len(epoch_prior_losses)\n",
    "#     train_losses.append(avg_loss)\n",
    "#     prior_losses.append(avg_prior_loss)\n",
    "    \n",
    "#     print(f\"\\nEpoch {epoch+1}/{num_epochs} completed, \"\n",
    "#         f\"Avg Loss: {avg_loss:.4f}, \"\n",
    "#         f\"Avg Prior Loss: {avg_prior_loss:.4f}\")\n",
    "    \n",
    "#     # Save checkpoints\n",
    "#     if (epoch + 1) % save_interval == 0:\n",
    "#         checkpoint_path = os.path.join(save_dir, f\"checkpoint_epoch_{epoch+1}.pt\")\n",
    "#         torch.save({\n",
    "#             'epoch': epoch + 1,\n",
    "#             'encoder_state_dict': model.encoder.state_dict(),\n",
    "#             'unet_state_dict': model.u_net.state_dict(),\n",
    "#             'optimizer_state_dict': optimizer.state_dict(),\n",
    "#             'loss': avg_loss,\n",
    "#         }, checkpoint_path)\n",
    "#         print(f\"Saved checkpoint to {checkpoint_path}\")\n",
    "\n",
    "#     if (epoch + 1) >= 3000 and (epoch + 1) % 1 == 0 and predict_x0_viz is not None:\n",
    "#         # show image\n",
    "#         draw_image(predict_x0_viz)\n",
    "#         draw_image(noise)\n",
    "\n",
    "#     if (epoch + 1) >= 1000 and (epoch + 1) % 10 == 0:\n",
    "#         print ((epoch + 1) % 10)\n",
    "#         model.eval()\n",
    "        \n",
    "#         image = sav_images\n",
    "\n",
    "#         start_noise = image\n",
    "#         start_noise = torch.rand_like(start_noise) \n",
    "#         draw_image(start_noise)\n",
    "#         model.evaluate_ddim(image, start_noise, denoise_steps=30)\n",
    "#         model.train()\n",
    "\n",
    "#         torch.cuda.empty_cache()\n",
    "#         gc.collect()\n",
    "\n",
    "\n",
    "#     # Step the learning rate scheduler\n",
    "#     scheduler.step()\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f587fe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "\n",
    "# def test_ddim():\n",
    "#     model.eval()\n",
    "#     image = sample_batch.cuda()\n",
    "#     start_noise = torch.rand_like(image) \n",
    "#     draw_image(image)\n",
    "#     model.evaluate_ddim(image, start_noise, denoise_steps=30)\n",
    "\n",
    "# test_ddim()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
