{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22278f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "# sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), './')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9a101c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "COCO_ROOT_DIR = r\"D:\\temp_dataset\\coco\\images\\train2017.1\\train2017\"  # Thư mục gốc chứa COCO\n",
    "TRAIN_IMAGES_SUBDIR = \"\"     # Thư mục con chứa ảnh train2017\n",
    "TRAIN_ANNOTATIONS_FILENAME = \"captions_train2017.json\" # File chú thích cho train2017\n",
    "\n",
    "PATCH_SIZE = (256, 256)  # Kích thước patch mong muốn (height, width)\n",
    "BATCH_SIZE = 64          # Kích thước batch cho DataLoader\n",
    "NUM_WORKERS = 4         # Số luồng để tải dữ liệu\n",
    "\n",
    "# --- Xây dựng đường dẫn đầy đủ ---\n",
    "train_images_path = os.path.join(COCO_ROOT_DIR, TRAIN_IMAGES_SUBDIR)\n",
    "train_annotations_path = os.path.join(r\"D:\\temp_dataset\\coco\\images\\train2017.1\\annotations_trainval2017\\annotations\", TRAIN_ANNOTATIONS_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a3cc2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 images in D:\\ds_coco_patches\n",
      "Batch size: 1\n",
      "Batch shape: torch.Size([1, 3, 256, 256])\n",
      "Estimated memory usage of the batch on CUDA: 0.75 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ------------------------ Example usage ------------------------\n",
    "\n",
    "from data_loader import get_coco_patches_loader\n",
    "\n",
    "\n",
    "BATCH_SIZE = 1  # Adjust based on your GPU memory\n",
    "NUM_WORKERS = 4  # 0 = no multiprocessing\n",
    "MAX_IMAGES = 1  # Limit to first 3000 images\n",
    "\n",
    "train_loader, train_dataset = get_coco_patches_loader(\n",
    "    data_dir=\"D:\\\\ds_coco_patches\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    pin_memory=True,\n",
    "    num_workers=4 if torch.cuda.is_available() else 0,\n",
    "    shuffle=True,\n",
    "    cache_size=0,\n",
    "    max_images=MAX_IMAGES\n",
    ")\n",
    "\n",
    "# Move a batch to CUDA and measure its size\n",
    "for images in train_loader:\n",
    "    images = images.cuda()  # Move to CUDA\n",
    "    print(f\"Batch size: {images.size(0)}\")\n",
    "    print(f\"Batch shape: {images.shape}\")  # Should be [B, 3, H, W]\n",
    "    \n",
    "    # Calculate memory usage\n",
    "    batch_memory = images.element_size() * images.nelement() / (1024 ** 2)\n",
    "    print(f\"Estimated memory usage of the batch on CUDA: {batch_memory:.2f} MB\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0699def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([1, 3, 256, 256])\n",
      "Estimated batch size in MB: 0.75\n"
     ]
    }
   ],
   "source": [
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"Batch shape: {sample_batch.shape}\")\n",
    "print(f\"Estimated batch size in MB: {sample_batch.element_size() * sample_batch.nelement() / 1024**2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b1e0375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing HyperPrior on device: cuda\n",
      "DiffusionManager initialized on device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python installation\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\python installation\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from compressor.compressor import Compressor\n",
    "from decompressor.diffusion_manager import DiffusionManager\n",
    "from decompressor.unet_module import UnetModule\n",
    "\n",
    "\n",
    "# Initialize the compressor\n",
    "compressor = Compressor(\n",
    "    channel_multiplier=[1, 3, 3, 12, 52, 64],\n",
    "    hyperprior_channel_multiplier=[64,64,64]\n",
    ")\n",
    "\n",
    "# Generate a dummy tensor to simulate input data\n",
    "dummy_tensor = torch.randn(64, 3, 256, 256).cuda()\n",
    "\n",
    "# Pass the dummy tensor through the compressor\n",
    "output_dict = compressor(dummy_tensor)\n",
    "\n",
    "# Extract the shapes of the output tensors\n",
    "output_shapes = [output.shape[1] for output in output_dict['output']]\n",
    "\n",
    "# Initialize the UNet module with the extracted channel dimensions\n",
    "unet_module = UnetModule(\n",
    "    base_channels=3,\n",
    "    context_channels=output_shapes\n",
    ")\n",
    "\n",
    "del dummy_tensor\n",
    "\n",
    "model = DiffusionManager(\n",
    "    encoder=compressor,\n",
    "    u_net=unet_module,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf296129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 41.35 MB\n"
     ]
    }
   ],
   "source": [
    "def get_model_size_in_mb(model):\n",
    "    total_size = sum(param.element_size() * param.nelement() for param in model.parameters())\n",
    "    total_size += sum(buffer.element_size() * buffer.nelement() for buffer in model.buffers())\n",
    "    return total_size / (1024 ** 2)  # Convert bytes to MB\n",
    "\n",
    "model_size_mb = get_model_size_in_mb(model)\n",
    "print(f\"Model size: {model_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40dc12ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2241059a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_image(input, save_dir=\"visualizations\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    image = input[0].cpu().detach().numpy()\n",
    "    image = np.transpose(image, (1, 2, 0))\n",
    "    image = (image * 255).astype(np.uint8)\n",
    "    image = Image.fromarray(image)\n",
    "    # Generate a unique filename using timestamp\n",
    "    filename = f\"image_{int(time.time() * 1000)}.png\"\n",
    "    save_path = os.path.join(save_dir, filename)\n",
    "    image.save(save_path)\n",
    "    print(f\"Image saved to {save_path}\")\n",
    "    image.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3ff6f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.autograd.set_detect_anomaly(True)\n",
    "# from torch.utils.data import DataLoader\n",
    "# import tqdm\n",
    "# import datetime\n",
    "# import os\n",
    "# from PIL import Image\n",
    "\n",
    "# import pynvml\n",
    "# pynvml.nvmlInit()\n",
    "# handle = pynvml.nvmlDeviceGetHandleByIndex(torch.cuda.current_device())\n",
    "\n",
    "# def print_gpu_mem(tag=\"\"):\n",
    "#     info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "#     allocated = torch.cuda.memory_allocated() / 1024**2\n",
    "#     reserved = torch.cuda.memory_reserved() / 1024**2\n",
    "#     print(f\"[{tag}] GPU Mem Used (NVML): {info.used / 1024**2:.2f} MB | Allocated (PyTorch): {allocated:.2f} MB | Reserved (PyTorch): {reserved:.2f} MB\")\n",
    "\n",
    "# NUM_WORKERS = 0 if torch.cuda.is_available() else 0\n",
    "\n",
    "# # Training parameters\n",
    "# num_epochs = 20000\n",
    "# log_interval = 50  # Log every 50 batches\n",
    "# save_interval = 100   # Save checkpoint every epoch\n",
    "\n",
    "# # Create optimizer for both compressor and UNet - REMOVED mixed precision components\n",
    "# optimizer = torch.optim.AdamW(\n",
    "#     [\n",
    "#         {'params': model.encoder.parameters()},\n",
    "#         {'params': model.u_net.parameters()}\n",
    "#     ],\n",
    "#     lr=2e-3,  # Typical for diffusion models\n",
    "#     weight_decay=1e-3,\n",
    "#     betas=(0.9, 0.999)\n",
    "# )\n",
    "\n",
    "# # Cosine annealing scheduler is common for diffusion models\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "#     optimizer,\n",
    "#     T_max=num_epochs,\n",
    "#     eta_min=1e-5\n",
    "# )\n",
    "\n",
    "# # Create directory for saving checkpoints\n",
    "# save_dir = os.path.join(\"checkpoints\", \n",
    "#                         f\"cdc_training_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "# os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# # Training statistics\n",
    "# train_losses = []\n",
    "# prior_losses = []\n",
    "\n",
    "# # Move models to CUDA\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = model.to(device)\n",
    "\n",
    "# print(f\"Starting training on {device}\")\n",
    "# print(f\"Training for {num_epochs} epochs with batch size {BATCH_SIZE}\")\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in range(num_epochs):\n",
    "#     train_dataset.clear_cache()  # Clear cache at the start of each epoch\n",
    "#     epoch_losses = []\n",
    "#     epoch_prior_losses = []\n",
    "    \n",
    "#     # Create tqdm progress bar\n",
    "#     progress_bar = tqdm.tqdm(enumerate(train_loader), total=len(train_loader),\n",
    "#                             desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "#     model.train()  # Set model to training mode\n",
    "#     predict_x0_viz = None\n",
    "#     noise = None\n",
    "\n",
    "#     sav_images = None\n",
    "    \n",
    "#     for batch_idx, images in progress_bar:\n",
    "#         images = images.to(device)  # Move images to GPU\n",
    "#         sav_images = images\n",
    "#         # Zero gradients\n",
    "#         optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "#         # Normal forward pass without autocast\n",
    "#         total_loss, prior_loss, estimate_x0, loss_dict, noise_add = model(images)\n",
    "#         # Save a batch for visualization at end of epoch\n",
    "#         if batch_idx == 0:\n",
    "#             predict_x0_viz = estimate_x0    \n",
    "#             noise = noise_add\n",
    "        \n",
    "#         # Check for NaN in losses\n",
    "#         if torch.isnan(total_loss).any() or torch.isnan(prior_loss).any():\n",
    "#             print(f\"NaN detected in losses at batch {batch_idx}. Skipping this batch.\")\n",
    "#             continue\n",
    "\n",
    "#         # combined_loss = total_loss + prior_loss\n",
    "\n",
    "#         # # Check for NaN in loss\n",
    "#         # if torch.isnan(combined_loss).any():\n",
    "#         #     print(f\"NaN detected in combined_loss at batch {batch_idx}. Skipping this batch.\")\n",
    "#         #     continue\n",
    "\n",
    "#         # Standard backward pass\n",
    "#         # combined_loss.backward()\n",
    "#         total_loss.backward()\n",
    "#         # prior_loss.backward()\n",
    "\n",
    "#         # Clip gradients to prevent exploding gradients\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "#         # Check for NaN in gradients\n",
    "#         has_nan_grad = False\n",
    "#         for name, param in model.named_parameters():\n",
    "#             if param.grad is not None and torch.isnan(param.grad).any():\n",
    "#                 print(f\"NaN detected in gradients of parameter {name}. Skipping this batch.\")\n",
    "#                 has_nan_grad = True\n",
    "#                 break\n",
    "        \n",
    "#         if has_nan_grad:\n",
    "#             continue\n",
    "\n",
    "#         # Standard optimizer step\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         loss_item = total_loss.item()\n",
    "#         prior_loss_item = prior_loss.item()\n",
    "#         epoch_losses.append(loss_item)\n",
    "#         epoch_prior_losses.append(prior_loss_item)\n",
    "        \n",
    "#         # Update progress bar\n",
    "#         progress_bar.set_postfix({\n",
    "#             'loss': f\"{epoch_losses[-1]:.4f}\",\n",
    "#             'prior_loss': f\"{epoch_prior_losses[-1]:.4f}\",\n",
    "#             **{key: f\"{value:.4f}\" for key, value in loss_dict.items()}\n",
    "#         })\n",
    "        \n",
    "#         # Log periodically\n",
    "#         if batch_idx % log_interval == 0 and batch_idx > 0:\n",
    "#             print(f\"\\nBatch {batch_idx}/{len(train_loader)}, \"\n",
    "#                 f\"Loss: {sum(epoch_losses[-log_interval:]) / log_interval:.4f}, \"\n",
    "#                 f\"Prior Loss: {sum(epoch_prior_losses[-log_interval:]) / log_interval:.4f}\")\n",
    "\n",
    "#         del total_loss, prior_loss, loss_item, prior_loss_item\n",
    "    \n",
    "#     # Calculate average epoch loss\n",
    "#     avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "#     avg_prior_loss = sum(epoch_prior_losses) / len(epoch_prior_losses)\n",
    "#     train_losses.append(avg_loss)\n",
    "#     prior_losses.append(avg_prior_loss)\n",
    "    \n",
    "#     print(f\"\\nEpoch {epoch+1}/{num_epochs} completed, \"\n",
    "#         f\"Avg Loss: {avg_loss:.4f}, \"\n",
    "#         f\"Avg Prior Loss: {avg_prior_loss:.4f}\")\n",
    "    \n",
    "#     # Save checkpoints\n",
    "#     if (epoch + 1) % save_interval == 0:\n",
    "#         checkpoint_path = os.path.join(save_dir, f\"checkpoint_epoch_{epoch+1}.pt\")\n",
    "#         torch.save({\n",
    "#             'epoch': epoch + 1,\n",
    "#             'encoder_state_dict': model.encoder.state_dict(),\n",
    "#             'unet_state_dict': model.u_net.state_dict(),\n",
    "#             'optimizer_state_dict': optimizer.state_dict(),\n",
    "#             'loss': avg_loss,\n",
    "#         }, checkpoint_path)\n",
    "#         print(f\"Saved checkpoint to {checkpoint_path}\")\n",
    "\n",
    "#     if (epoch + 1) >= 3000 and (epoch + 1) % 1 == 0 and predict_x0_viz is not None:\n",
    "#         # show image\n",
    "#         draw_image(predict_x0_viz)\n",
    "#         draw_image(noise)\n",
    "\n",
    "#     if (epoch + 1) >= 4000 and (epoch + 1) % 100 == 0:\n",
    "#         print ((epoch + 1) % 10)\n",
    "#         model.eval()\n",
    "        \n",
    "#         image = sav_images\n",
    "\n",
    "#         start_noise = image\n",
    "#         start_noise = torch.rand_like(start_noise) \n",
    "#         draw_image(start_noise)\n",
    "#         model.evaluate_ddim(image, start_noise, denoise_steps=30)\n",
    "#         model.train()\n",
    "\n",
    "#         torch.cuda.empty_cache()\n",
    "#         gc.collect()\n",
    "\n",
    "\n",
    "#     # Step the learning rate scheduler\n",
    "#     scheduler.step()\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ab56d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint from E:\\DUT Courses\\Academic year 4\\semester 2\\PBL\\PBL7 CDC Compression\\checkpoints\\cdc_training_20250519_013027\\checkpoint_epoch_4000.pt\n"
     ]
    }
   ],
   "source": [
    "save_dir = r\"E:\\DUT Courses\\Academic year 4\\semester 2\\PBL\\PBL7 CDC Compression\\checkpoints\\cdc_training_20250519_013027\"\n",
    "\n",
    "# Load checkpoint \n",
    "checkpoint_path = os.path.join(save_dir, f\"checkpoint_epoch_{4000}.pt\")\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "model.encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "model.u_net.load_state_dict(checkpoint['unet_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "model.train()  # Set model to training mode\n",
    "print(f\"Loaded checkpoint from {checkpoint_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
