{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22278f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "# sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), './')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9a101c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "COCO_ROOT_DIR = r\"D:\\temp_dataset\\coco\\images\\train2017.1\\train2017\"  # Thư mục gốc chứa COCO\n",
    "TRAIN_IMAGES_SUBDIR = \"\"     # Thư mục con chứa ảnh train2017\n",
    "TRAIN_ANNOTATIONS_FILENAME = \"captions_train2017.json\" # File chú thích cho train2017\n",
    "\n",
    "PATCH_SIZE = (256, 256)  # Kích thước patch mong muốn (height, width)\n",
    "BATCH_SIZE = 32          # Kích thước batch cho DataLoader\n",
    "NUM_WORKERS = 4         # Số luồng để tải dữ liệu\n",
    "\n",
    "# --- Xây dựng đường dẫn đầy đủ ---\n",
    "train_images_path = os.path.join(COCO_ROOT_DIR, TRAIN_IMAGES_SUBDIR)\n",
    "train_annotations_path = os.path.join(r\"D:\\temp_dataset\\coco\\images\\train2017.1\\annotations_trainval2017\\annotations\", TRAIN_ANNOTATIONS_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a3cc2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 images in D:\\ds_coco_patches\n",
      "Batch size: 1\n",
      "Batch shape: torch.Size([1, 3, 256, 256])\n",
      "Estimated memory usage of the batch on CUDA: 0.75 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ------------------------ Example usage ------------------------\n",
    "\n",
    "from data_loader import get_coco_patches_loader\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32  # Adjust based on your GPU memory\n",
    "NUM_WORKERS = 4  # 0 = no multiprocessing\n",
    "MAX_IMAGES = 1  # Limit to first 3000 images\n",
    "\n",
    "train_loader, train_dataset = get_coco_patches_loader(\n",
    "    data_dir=\"D:\\\\ds_coco_patches\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    pin_memory=True,\n",
    "    num_workers=0 if torch.cuda.is_available() else 0,\n",
    "    shuffle=True,\n",
    "    cache_size=0,\n",
    "    max_images=MAX_IMAGES\n",
    ")\n",
    "\n",
    "# Move a batch to CUDA and measure its size\n",
    "for images in train_loader:\n",
    "    images = images.cuda()  # Move to CUDA\n",
    "    print(f\"Batch size: {images.size(0)}\")\n",
    "    print(f\"Batch shape: {images.shape}\")  # Should be [B, 3, H, W]\n",
    "    \n",
    "    # Calculate memory usage\n",
    "    batch_memory = images.element_size() * images.nelement() / (1024 ** 2)\n",
    "    print(f\"Estimated memory usage of the batch on CUDA: {batch_memory:.2f} MB\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0699def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([1, 3, 256, 256])\n",
      "Estimated batch size in MB: 0.75\n"
     ]
    }
   ],
   "source": [
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"Batch shape: {sample_batch.shape}\")\n",
    "print(f\"Estimated batch size in MB: {sample_batch.element_size() * sample_batch.nelement() / 1024**2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ccad66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet Encoder Input-Output Pairs: [(3, 32), (32, 64), (64, 96), (96, 128), (128, 160), (160, 192), (192, 224), (224, 256)]\n",
      "UNet Encoder Channels Pairs: [(6, 32), (64, 64), (128, 96), (192, 128), (256, 160), (320, 192), (192, 224), (224, 256)]\n",
      "UNet Decoder Channels Pairs: [(512, 224), (448, 192), (384, 160), (320, 128), (256, 96), (192, 64), (128, 32), (64, 3)]\n",
      "Context Channels: [3, 32, 64, 96, 128, 160]\n",
      "UNet Encoder Input-Output Pairs: [(3, 32), (32, 64), (64, 96), (96, 128), (128, 160), (160, 192), (192, 224), (224, 256)]\n",
      "UNet Encoder Channels Pairs: [(6, 32), (64, 64), (128, 96), (192, 128), (256, 160), (320, 192), (192, 224), (224, 256)]\n",
      "UNet Decoder Channels Pairs: [(512, 224), (448, 192), (384, 160), (320, 128), (256, 96), (192, 64), (128, 32), (64, 3)]\n",
      "Context Channels: [3, 32, 64, 96, 128, 160]\n"
     ]
    }
   ],
   "source": [
    "# from cdc_trainable import CDCTrainable\n",
    "from cdc_trainable import CDCTrainable\n",
    "from decompressor.diffusion_manager import DiffusionManager\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CDCTrainable(device=device)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b1e0375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from compressor.compressor import Compressor\n",
    "# from decompressor.diffusion_manager import DiffusionManager\n",
    "# from decompressor.unet_module import UnetModule\n",
    "\n",
    "\n",
    "# # Initialize the compressor\n",
    "# compressor = Compressor(\n",
    "#     channel_multiplier=[1, 3, 3, 12, 52, 64],\n",
    "#     hyperprior_channel_multiplier=[64,64,64]\n",
    "# )\n",
    "\n",
    "# # Generate a dummy tensor to simulate input data\n",
    "# dummy_tensor = torch.randn(64, 3, 256, 256).cuda()\n",
    "\n",
    "# # Pass the dummy tensor through the compressor\n",
    "# output_dict = compressor(dummy_tensor)\n",
    "\n",
    "# # Extract the shapes of the output tensors\n",
    "# output_shapes = [output.shape[1] for output in output_dict['output']]\n",
    "\n",
    "# # Initialize the UNet module with the extracted channel dimensions\n",
    "# unet_module = UnetModule(\n",
    "#     base_channels=3,\n",
    "#     context_channels=output_shapes\n",
    "# )\n",
    "\n",
    "# del dummy_tensor\n",
    "\n",
    "# model = DiffusionManager(\n",
    "#     encoder=compressor,\n",
    "#     u_net=unet_module,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf296129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 187.36 MB\n"
     ]
    }
   ],
   "source": [
    "def get_model_size_in_mb(model):\n",
    "    total_size = sum(param.element_size() * param.nelement() for param in model.parameters())\n",
    "    total_size += sum(buffer.element_size() * buffer.nelement() for buffer in model.buffers())\n",
    "    return total_size / (1024 ** 2)  # Convert bytes to MB\n",
    "\n",
    "model_size_mb = get_model_size_in_mb(model)\n",
    "print(f\"Model size: {model_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40dc12ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2241059a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_image(input, save_dir=\"visualizations\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    image = input[0].cpu().detach().numpy()\n",
    "    image = np.transpose(image, (1, 2, 0))\n",
    "    image = (image * 255).astype(np.uint8)\n",
    "    image = Image.fromarray(image)\n",
    "    # Generate a unique filename using timestamp\n",
    "    filename = f\"image_{int(time.time() * 1000)}.png\"\n",
    "    save_path = os.path.join(save_dir, filename)\n",
    "    image.save(save_path)\n",
    "    # print(f\"Image saved to {save_path}\")\n",
    "    image.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9b51fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "from torch.amp import autocast, GradScaler\n",
    "import tqdm \n",
    "import datetime\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "NUM_WORKERS = 0 if not torch.cuda.is_available() else 0 \n",
    "\n",
    "num_epochs = 100000\n",
    "log_interval = 100\n",
    "save_interval = 1000\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=5e-4,  # Typical for diffusion models\n",
    "    weight_decay=1e-4,\n",
    "    betas=(0.9, 0.999)\n",
    ")\n",
    "\n",
    "# Cosine annealing scheduler is common for diffusion models\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=num_epochs,\n",
    "    eta_min=1e-7\n",
    ")\n",
    "\n",
    "save_dir = os.path.join(\"checkpoints\", \n",
    "                        f\"cdc_training_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "train_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cd325df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_dir = r\"E:\\DUT Courses\\Academic year 4\\semester 2\\PBL\\PBL7 CDC Compression\\checkpoints\\cdc_training_20250605_133848\"\n",
    "\n",
    "# # Load checkpoint \n",
    "# checkpoint_path = os.path.join(save_dir, f\"checkpoint_epoch_{20000}.pt\")\n",
    "# checkpoint = torch.load(checkpoint_path)\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# epochs = checkpoint['epoch']\n",
    "# # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# model.train()  # Set model to training mode\n",
    "# print(f\"Loaded checkpoint from {checkpoint_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de3a6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100000: 100%|██████████| 1/1 [00:02<00:00,  2.71s/it, loss=2.3447, mse_loss=1.5789, reconstruction_loss=4.1313, bpp_loss=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 completed - Avg Loss: 2.3447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100000: 100%|██████████| 1/1 [00:01<00:00,  1.54s/it, loss=1.0428, mse_loss=1.4124, reconstruction_loss=0.1769, bpp_loss=0.0010]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 completed - Avg Loss: 1.0428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100000: 100%|██████████| 1/1 [00:01<00:00,  1.49s/it, loss=0.9642, mse_loss=1.3212, reconstruction_loss=0.1309, bpp_loss=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 completed - Avg Loss: 0.9642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100000: 100%|██████████| 1/1 [00:01<00:00,  1.57s/it, loss=0.8970, mse_loss=1.2587, reconstruction_loss=0.0523, bpp_loss=0.0002]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 completed - Avg Loss: 0.8970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100000: 100%|██████████| 1/1 [00:01<00:00,  1.62s/it, loss=1.7253, mse_loss=1.1906, reconstruction_loss=2.9723, bpp_loss=0.0002]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 completed - Avg Loss: 1.7253\n",
      "DDIM Step: 50/50\n",
      "DDIM Step: 49/50\n",
      "DDIM Step: 48/50\n",
      "DDIM Step: 47/50\n",
      "DDIM Step: 46/50\n",
      "DDIM Step: 45/50\n",
      "DDIM Step: 44/50\n",
      "DDIM Step: 43/50\n",
      "DDIM Step: 42/50\n",
      "DDIM Step: 41/50\n",
      "DDIM Step: 40/50\n",
      "DDIM Step: 39/50\n",
      "DDIM Step: 38/50\n",
      "DDIM Step: 37/50\n",
      "DDIM Step: 36/50\n",
      "DDIM Step: 35/50\n",
      "DDIM Step: 34/50\n",
      "DDIM Step: 33/50\n",
      "DDIM Step: 32/50\n",
      "DDIM Step: 31/50\n",
      "DDIM Step: 30/50\n",
      "DDIM Step: 29/50\n",
      "DDIM Step: 28/50\n",
      "DDIM Step: 27/50\n",
      "DDIM Step: 26/50\n",
      "DDIM Step: 25/50\n",
      "DDIM Step: 24/50\n",
      "DDIM Step: 23/50\n",
      "DDIM Step: 22/50\n",
      "DDIM Step: 21/50\n",
      "DDIM Step: 20/50\n",
      "DDIM Step: 19/50\n",
      "DDIM Step: 18/50\n",
      "DDIM Step: 17/50\n",
      "DDIM Step: 16/50\n",
      "DDIM Step: 15/50\n",
      "DDIM Step: 14/50\n",
      "DDIM Step: 13/50\n",
      "DDIM Step: 12/50\n",
      "DDIM Step: 11/50\n",
      "DDIM Step: 10/50\n",
      "DDIM Step: 9/50\n",
      "DDIM Step: 8/50\n",
      "DDIM Step: 7/50\n",
      "DDIM Step: 6/50\n",
      "DDIM Step: 5/50\n",
      "DDIM Step: 4/50\n",
      "DDIM Step: 3/50\n",
      "DDIM Step: 2/50\n",
      "DDIM Step: 1/50\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    train_dataset.clear_cache()\n",
    "    epoch_losses = []\n",
    "    model.train()\n",
    "\n",
    "    progress_bar = tqdm.tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    image_for_viz = None\n",
    "    predicted_x0 = None\n",
    "    test_image = None\n",
    "\n",
    "    for batch_idx, images in progress_bar:\n",
    "        images = images.to(device)\n",
    "        test_image = images[0].unsqueeze(0)  # For visualization purposes\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with autocast(device_type='cuda', enabled=torch.cuda.is_available()):\n",
    "            loss, x0_img, image_noise,  mse_loss, reconstruction_loss, bpp_loss  = model(images)\n",
    "\n",
    "        if batch_idx == 0:\n",
    "            predicted_x0 = x0_img\n",
    "            image_for_viz = image_noise[0].unsqueeze(0)\n",
    "\n",
    "        if torch.isnan(loss).any():\n",
    "            print(f\"NaN in loss at batch {batch_idx}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        if any(torch.isnan(param.grad).any() for param in model.parameters() if param.grad is not None):\n",
    "            print(f\"NaN in gradients at batch {batch_idx}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_losses.append(loss.item())\n",
    "\n",
    "        progress_bar.set_postfix({'loss': f\"{epoch_losses[-1]:.4f}\", 'mse_loss': f\"{mse_loss.item():.4f}\", 'reconstruction_loss': f\"{reconstruction_loss.item():.4f}\", 'bpp_loss': f\"{bpp_loss.item():.4f}\"})\n",
    "\n",
    "        if batch_idx % log_interval == 0 and batch_idx > 0:\n",
    "            avg_recent_loss = sum(epoch_losses[-log_interval:]) / log_interval\n",
    "            print(f\"Batch {batch_idx}/{len(train_loader)} - Avg Loss: {avg_recent_loss:.4f}\")\n",
    "\n",
    "        del loss\n",
    "\n",
    "    avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    print(f\"\\nEpoch {epoch+1} completed - Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % save_interval == 0:\n",
    "        checkpoint_path = os.path.join(save_dir, f\"checkpoint_epoch_{epoch+1}.pt\")\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "    # if (epoch) >= 10000 and predicted_x0 is not None and (epoch + 1) % 500 == 0:\n",
    "    draw_image(predicted_x0)\n",
    "    draw_image(image_for_viz)\n",
    "\n",
    "    if (epoch) >= 10000 and (epoch + 1) % 500 == 0:\n",
    "        model.eval()\n",
    "        # draw_image(test_image, save_dir=\"visualizations_test\")\n",
    "        model.ddim_forward(test_image)\n",
    "        model.train()\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        break\n",
    "\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ea504a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "\n",
    "# images = next(iter(train_loader))\n",
    "\n",
    "# for epoch in range(0, 50):\n",
    "#     train_dataset.clear_cache()\n",
    "#     epoch_losses = []\n",
    "\n",
    "#     progress_bar = tqdm.tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "#     image_for_viz = None\n",
    "#     predicted_x0 = None\n",
    "#     test_image = None\n",
    "\n",
    "\n",
    "#     images = images.to(device)\n",
    "#     test_image = images[0].unsqueeze(0)  # For visualization purposes\n",
    "\n",
    "#     optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "#     with autocast(device_type='cuda', enabled=torch.cuda.is_available()):\n",
    "#         model.forward_fake(images, denoise_steps=50)\n",
    "#         break\n",
    "\n",
    "#     # if True:\n",
    "#     #     predicted_x0 = x0_img\n",
    "#     #     image_for_viz = image_noise[0].unsqueeze(0)\n",
    "\n",
    "#     # # if (epoch) >= 10000 and predicted_x0 is not None and (epoch + 1) % 500 == 0:\n",
    "#     # draw_image(predicted_x0)\n",
    "#     # draw_image(image_for_viz)\n",
    "\n",
    "#     # images = predicted_x0\n",
    "\n",
    "#     # if (epoch) >= 1000 and (epoch + 1) % 100 == 0:\n",
    "#     #     model.eval()\n",
    "#     #     # draw_image(test_image, save_dir=\"visualizations_test\")\n",
    "#     #     model.ddim_forward(test_image, denoise_steps=50)\n",
    "#     #     model.train()\n",
    "\n",
    "#     #     torch.cuda.empty_cache()\n",
    "#     #     gc.collect()\n",
    "#     #     # break\n",
    "\n",
    "#     scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e47f744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model saved for evaluation at checkpoints\\cdc_training_20250607_111541\\final_model.pt\n"
     ]
    }
   ],
   "source": [
    "# Save model as final (for evaluation/inference)\n",
    "model.eval()\n",
    "final_checkpoint_path = os.path.join(save_dir, \"final_model.pt\")\n",
    "torch.save({\n",
    "    'epoch': num_epochs,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "}, final_checkpoint_path)\n",
    "print(f\"Final model saved for evaluation at {final_checkpoint_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
